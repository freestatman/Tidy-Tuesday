---
title: "TidyTuesdayAnimalCrossing"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

```{r}
library(tidyverse)
library(tidytext)
library(tidymodels)
user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
```

```{r}
#brief look at grade distribution 
user_reviews %>% 
  count(grade) %>% 
  ggplot(aes(x = grade, y = n)) + geom_col()
```

```{r, fig.height=15, fig.width=15}
#Clustering of the top 25 words for each grade using pCA and visualizing it 
sparse_df <- user_reviews %>% 
  select(grade, text) %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  group_by(grade) %>% 
  top_n(n, n = 25) %>% 
  ungroup() %>% 
  cast_sparse(row = grade, column = word, value = n)

library(irlba)
pca_text <- prcomp_irlba(sparse_df, n = 4, scale. = TRUE)

pca_text$center %>% 
  tidy() %>% 
  select(names) %>% 
  cbind(pca_text$rotation) %>% 
  ggplot(aes(x = PC1, y = PC2, label = names)) + 
  geom_point() + 
  geom_text()
```

```{r}
#using tf_idf
user_reviews %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  bind_tf_idf(word, grade, n) %>% 
  group_by(grade) %>% 
  top_n(tf_idf, n =5) %>% 
  ungroup() %>% 
  mutate(grade = as.factor(grade)) %>% 
  ggplot(aes(x = reorder_within(word, tf_idf, grade), y = tf_idf, fill = grade)) + 
  geom_col() + 
  scale_x_reordered() + 
  coord_flip() + 
  facet_wrap(~grade, scales = "free") + 
  theme(legend.position = 'none')

```




```{r}
#Using weighted log odds 
library(tidylo)
user_reviews %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  bind_log_odds(grade, word, n) %>% 
  group_by(grade) %>% 
  top_n(log_odds, n =5) %>% 
  ungroup() %>% 
  mutate(grade = as.factor(grade)) %>% 
  ggplot(aes(x = reorder_within(word, log_odds, grade), y = log_odds, fill = grade)) + 
  geom_col() + 
  scale_x_reordered() + 
  coord_flip() + 
  facet_wrap(~grade, scales = "free") + 
  theme(legend.position = 'none')

```

```{r}
#Partition data 
set.seed(42)

tidy_data <- user_reviews %>% select(-user_name)

tidy_split <- initial_split(tidy_data, p = .8)
tidy_train <- training(tidy_split)
tidy_test <- testing(tidy_split)


tidy_split$in_id
```


```{r}

library(textrecipes)

text_recipe <- recipe(grade~text, data = tidy_train) %>% 
  step_tokenize(text) %>% 
  step_stopwords(text) %>% 
  step_tokenfilter(text, max_tokens = 500) %>% 
  step_tf(text)

text_prep <- text_recipe %>% prep()

cross_validation <- vfold_cv(tidy_train, v = 10)

wf <- workflow() %>% 
  add_recipe(text_recipe)

```

```{r}

lasso_model <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

lasso_grid <- grid_regular(penalty(), levels = 10)

lasso_tune <- tune_grid(
  wf %>% add_model(lasso_model),
  resamples = cross_validation,
  grid = lasso_grid
)

lasso_tune %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + geom_line() + facet_wrap(~.metric, scales = "free")

lasso_best_tune <- lasso_tune %>% select_best("rmse")
  
final_lasso_model <- finalize_model(lasso_model, lasso_best_tune)  

lasso_wf <- workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(final_lasso_model)

lasso_eval <- lasso_wf %>% last_fit(tidy_split)
lasso_eval %>% collect_metrics()
```


```{r}

random_forest_model <- rand_forest(mtry = 25,
                                   trees = 1000,
                                   min_n = 20) %>% 
  set_mode("regression") %>% 
  set_engine("randomForest")

random_forest_tune <- fit_resamples(
  random_forest_model,
  text_recipe,
  cross_validation
)

random_forest_tune %>% 
  collect_metrics()


final_rf_wf <- workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(random_forest_model) 

final_rf_eval <- final_rf_wf %>% last_fit(tidy_split)
```

```{r}
#RF has better performance than lasso 
final_rf_eval %>% collect_metrics() %>% mutate(model = "rf") %>% 
  rbind(lasso_eval %>% collect_metrics() %>% mutate(model = "lasso")) %>% 
  ggplot(aes(x = model, y = .estimate, fill = model)) + geom_col() + facet_wrap(~.metric, scales = "free")
```


```{r}
library(keras)
keras_df <- user_reviews %>% select(-user_name)

max_features <- 2000

tokenizer <- text_tokenizer(num_words = max_features) %>% 
  fit_text_tokenizer(keras_df$text)

one_hot_results <- texts_to_matrix(tokenizer = tokenizer, keras_df$text, mode = "binary")

word_index <- tokenizer$word_index
```

```{r}
#Create the train and test inputs and outputs 
#Also apply padding to the inputs 
x_train <- one_hot_results[tidy_split$in_id,]
x_test <- one_hot_results[-tidy_split$in_id,]

y_train <- keras_df$grade[tidy_split$in_id]
y_test <- keras_df$grade[-tidy_split$in_id]


maxlen <- 128
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```


```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features,
                  output_dim = 16,
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 1)


model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
)
 

history <- model %>% 
  fit(
    x_train,
    y_train,
    validation_split = .2,
    epochs = 100,
    batch_size = 64
  )



```



```{r}
plot(history)
```


```{r}

evaluate(model,
         x_test,
         y_test)


```














